{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU5t1+G48QeMffOJt8AEHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeCraftIA/e_commerse_scraping/blob/main/sneakerhousearg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required libraries and dependences"
      ],
      "metadata": {
        "id": "JER39Fg1sb9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPNe-YIBsWpA",
        "outputId": "44d6fc84-be1e-4429-a4af-e9bb31cd2bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,974 kB]\n",
            "Fetched 3,577 kB in 1s (2,430 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.21.2-2ubuntu1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "You might want to run 'apt --fix-broken install' to correct these.\n",
            "The following packages have unmet dependencies:\n",
            " google-chrome-stable : Depends: libvulkan1 but it is not going to be installed\n",
            "\u001b[1;31mE: \u001b[0mUnmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).\u001b[0m\n",
            "--2024-04-11 18:24:18--  http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
            "Resolving archive.ubuntu.com (archive.ubuntu.com)... 91.189.91.83, 91.189.91.81, 91.189.91.82, ...\n",
            "Connecting to archive.ubuntu.com (archive.ubuntu.com)|91.189.91.83|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3708 (3.6K) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libu2f-udev_1.1.4-1_all.deb.1’\n",
            "\n",
            "libu2f-udev_1.1.4-1 100%[===================>]   3.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-11 18:24:18 (363 MB/s) - ‘libu2f-udev_1.1.4-1_all.deb.1’ saved [3708/3708]\n",
            "\n",
            "(Reading database ... 121870 files and directories currently installed.)\n",
            "Preparing to unpack libu2f-udev_1.1.4-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.4-1) over (1.1.4-1) ...\n",
            "Setting up libu2f-udev (1.1.4-1) ...\n",
            "--2024-04-11 18:24:18--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 142.250.141.190, 142.250.141.91, 142.250.141.93, ...\n",
            "Connecting to dl.google.com (dl.google.com)|142.250.141.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 107223784 (102M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb.1’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 102.26M   111MB/s    in 0.9s    \n",
            "\n",
            "2024-04-11 18:24:19 (111 MB/s) - ‘google-chrome-stable_current_amd64.deb.1’ saved [107223784/107223784]\n",
            "\n",
            "(Reading database ... 121870 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (123.0.6312.122-1) over (123.0.6312.122-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "--2024-04-11 18:24:32--  https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
            "Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 142.250.101.207, 142.251.2.207, 74.125.137.207, ...\n",
            "Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|142.250.101.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 304 Not Modified\n",
            "File ‘/tmp/chromedriver_linux64.zip’ not modified on server. Omitting download.\n",
            "\n",
            "Archive:  /tmp/chromedriver_linux64.zip\n",
            "  inflating: /tmp/chromedriver       \n",
            "  inflating: /tmp/LICENSE.chromedriver  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`\n",
        "wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
        "unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
        "chmod +x /tmp/chromedriver\n",
        "mv /tmp/chromedriver /usr/local/bin/chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc6mw5RUso7d",
        "outputId": "d26b0b41-c1fb-4387-fb47-057761fa5c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ubuntu.com] [Connecting\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com] [Connecting to ppa.launchpadcontent.net\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "You might want to run 'apt --fix-broken install' to correct these.\n",
            "The following packages have unmet dependencies:\n",
            " chromium-chromedriver : Depends: chromium-browser (>= 1:85.0.4183.83-0ubuntu2.22.04.1) but it is not going to be installed\n",
            " google-chrome-stable : Depends: libvulkan1 but it is not going to be installed\n",
            "\u001b[1;31mE: \u001b[0mUnmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).\u001b[0m\n",
            "cp: cannot stat '/usr/lib/chromium-browser/chromedriver': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install chromedriver-autoinstaller"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJLcJFgIss7U",
        "outputId": "14306e5a-f82b-43ea-83f0-92c849f9f398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.10.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.19.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from chromedriver-autoinstaller) (24.0)\n",
            "Installing collected packages: chromedriver-autoinstaller\n",
            "Successfully installed chromedriver-autoinstaller-0.6.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import re\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import chromedriver_autoinstaller\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# setup chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # ensure GUI is off\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\")\n",
        "\n",
        "# set path to chromedriver as per your configuration\n",
        "chromedriver_autoinstaller.install()\n",
        "\n",
        "url = 'https://www.sneakerhousearg.com/calendar'\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "driver.get(url)\n",
        "time.sleep(10)\n",
        "\n",
        "wait = WebDriverWait(driver, 10)\n",
        "wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"article\")))\n",
        "\n",
        "# Get the page source after waiting for dynamic content to load\n",
        "html_content = driver.page_source\n",
        "\n",
        "# Parse the HTML content with BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find all article elements containing shoe information\n",
        "shoe_articles = soup.find_all(\"article\")\n",
        "\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "# Initialize an empty list to store shoe data\n",
        "shoe_data_list = []\n",
        "\n",
        "# Iterate through each article element\n",
        "for article in tqdm(shoe_articles):\n",
        "    # Extract shoe name\n",
        "    shoe_name = article.find(\"h4\").text.strip()\n",
        "\n",
        "    # Extract image URL\n",
        "    image_url = \"https://www.sneakerhousearg.com\"+ article.find(\"img\")[\"src\"]\n",
        "\n",
        "    # Extract release date\n",
        "    release_date = article.find(\"span\", class_=\"absolute w-fit text-xs p-2 text-white bg-black font-medium cursor-default top-4 left-4\").text.strip()\n",
        "\n",
        "    # Extract shoe URL\n",
        "    shoe_url = \"https://www.sneakerhousearg.com\" + article.find(\"a\")[\"href\"]\n",
        "\n",
        "    # Visit the shoe URL to scrape additional information\n",
        "    driver.get(shoe_url)\n",
        "    time.sleep(10)  # Wait for the page to load\n",
        "\n",
        "    # Parse the HTML content of the shoe URL\n",
        "    shoe_html_content = driver.page_source\n",
        "    shoe_soup = BeautifulSoup(shoe_html_content, 'html.parser')\n",
        "\n",
        "    # Extract stores\n",
        "    store_section = shoe_soup.find(\"section\", class_=\"w-full xl:w-3/4 flex flex-col gap-2\")\n",
        "    store_links = store_section.find_all(\"a\")\n",
        "    stores = [link.text.strip() for link in store_links]\n",
        "\n",
        "    # Extract SKU\n",
        "    sku=\"\"\n",
        "    sku_section = shoe_soup.find(\"section\", class_=\"w-full xl:w-5/6 flex flex-col gap-2 overflow-hidden\")\n",
        "    all_div = sku_section.find_all(\"div\", class_=\"w-full py-2 flex border-[#8C8C8C] border-b capitalize items-center gap-1 last:border-b-0\")\n",
        "    for div in all_div:\n",
        "      h4 = div.find('h4',class_=\"w-1/2 font-semibold\").text.strip() #.find_all(\"p\")[-1].text.strip()\n",
        "      if h4==\"SKU\":\n",
        "        sku = div.find('p', class_=\"w-1/2 lg:text-sm\").text.strip()\n",
        "\n",
        "    # Create a dictionary to store the extracted information\n",
        "    shoe_data = {\n",
        "        \"Shoe Name\": shoe_name,\n",
        "        \"Image URL\": image_url,\n",
        "        \"Release Date\": release_date,\n",
        "        \"Shoe URL\": shoe_url,\n",
        "        \"Stores\": stores,\n",
        "        \"SKU\": sku\n",
        "    }\n",
        "\n",
        "    # Append the shoe data to the list\n",
        "    shoe_data_list.append(shoe_data)\n",
        "\n",
        "# Close the webdriver\n",
        "driver.quit()\n",
        "\n",
        "# Save the shoe data list to a JSON file\n",
        "output_file = \"shoe_data.json\"\n",
        "with open(output_file, \"w\") as f:\n",
        "    json.dump(shoe_data_list, f, indent=4)\n",
        "\n",
        "print(\"Data saved to\", output_file)\n",
        "\n",
        "from google.colab import files\n",
        "# Download the file\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KFqQkMgHsv9b",
        "outputId": "ec19a620-f9bb-403c-dfd0-e40418f0115a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [06:16<00:00, 10.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to shoe_data.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7e019f39-4876-447d-9377-927680b973fb\", \"shoe_data.json\", 18311)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Amazon scraping with selenium"
      ],
      "metadata": {
        "id": "QpoB0F1GZ3iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VUzCx-VZ6c8",
        "outputId": "fc705f4a-1a60-4e8c-e4d1-e3e3b1ddd347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/159.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m122.9/159.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def write_excel(items, path):\n",
        "    if len(items) == 0:\n",
        "        return\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    df = pd.DataFrame(items)\n",
        "\n",
        "    # Write DataFrame to Excel\n",
        "    with pd.ExcelWriter(path, engine='xlsxwriter', options={'strings_to_numbers': True}) as writer:\n",
        "        df.to_excel(writer, index=False, sheet_name='Sheet1')"
      ],
      "metadata": {
        "id": "Reh6M0h_aBnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def x3(soup):\n",
        "    review_box = soup.find('div', id=\"cm_cr-review_list\")\n",
        "\n",
        "    cards = review_box.find_all('div', class_=\"a-section a-spacing-large aok-relative\")\n",
        "\n",
        "    if len(cards) == 0:\n",
        "      return \"stop\"\n",
        "    else:\n",
        "      for card in cards:\n",
        "        rating = card.find('span', class_=\"a-icon-alt\")\n",
        "        if rating:\n",
        "          rating = rating.text.strip()\n",
        "\n",
        "        header = card.find('span', class_=\"a-size-base review-title-content a-color-base a-text-bold\")\n",
        "        if header:\n",
        "          header=header.text.strip()\n",
        "\n",
        "        body = card.find('div', class_=\"a-row a-spacing-small cr-truncated-content\")\n",
        "        if body:\n",
        "          body=body.text.strip()\n",
        "\n",
        "\n",
        "\n",
        "    #Return a Dictionary\n",
        "    return{\n",
        "        'Rating': rating,\n",
        "        'Header': header,\n",
        "        'Body': body,\n",
        "    }"
      ],
      "metadata": {
        "id": "eksGMEIwaGdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import re\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import chromedriver_autoinstaller\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# setup chrome options\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless') # ensure GUI is off\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 13_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1 Mobile/15E148 Safari/604.1\")\n",
        "chrome_options.add_argument(\"Accept-Encoding=gzip, deflate, br\")\n",
        "chrome_options.add_argument(\"Accept=text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\")\n",
        "chrome_options.add_argument(\"Connection=keep-alive\")\n",
        "chrome_options.add_argument(\"Upgrade-Insecure-Requests=1\")\n",
        "\n",
        "# set path to chromedriver as per your configuration\n",
        "chromedriver_autoinstaller.install()\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "def x4():\n",
        "    # Scrape data for each URL\n",
        "    results = []\n",
        "    #counter=0\n",
        "    for i in tqdm(range(1, 50)):\n",
        "        try:\n",
        "            url = \"https://www.amazon.com/Philips-Norelco-OneBlade-QP6531-70/product-reviews/B0BTZTBGS7/ref=cm_cr_arp_d_paging_btm_next_\" + str(i) + \"?ie=UTF8&reviewerType=all_reviews&pageNumber=\" + str(i)\n",
        "\n",
        "\n",
        "            driver.get(url)\n",
        "            time.sleep(15)\n",
        "\n",
        "            # Get the page source after waiting for dynamic content to load\n",
        "            html_content = driver.page_source\n",
        "\n",
        "            # Parse the HTML content with BeautifulSoup\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "            data=x3(soup)\n",
        "            if data == \"stop\":\n",
        "              break\n",
        "            results.append(data)\n",
        "            print(data)\n",
        "            time.sleep(10)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping data for URL: {url}\\nError: {e}\")\n",
        "            # Write the error URL to errors.txt\n",
        "            with open('errors.txt', 'a') as error_file:\n",
        "                error_file.write(url + '\\n')\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "43-q2SiRaP8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_csv_path = 'amazon.xlsx'\n",
        "\n",
        "# Get scraped data for each URL\n",
        "scraped_data = x4()\n",
        "driver.quit()\n",
        "\n",
        "# Write the results to a CSV file\n",
        "write_excel(scraped_data, output_csv_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "g15DiKsAaV67",
        "outputId": "0c2cca83-12e2-4398-c019-94f30a016805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/49 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Rating': '5.0 out of 5 stars', 'Header': 'Ease of use', 'Body': 'I like this trimmer because it’s easy to use for all hair types including body, head, and face. It’s easy to adjust the hair depth and it’s easy to clean. I would suggest buying it with the adjustable attachment. I’ve used this for 3 for body trimming & still haven’t...\\n      \\n\\n      \\n    \\n  \\nSee more'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 2/49 [00:41<15:26, 19.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping data for URL: https://www.amazon.com/Philips-Norelco-OneBlade-QP6531-70/product-reviews/B0BTZTBGS7/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2\n",
            "Error: 'NoneType' object has no attribute 'find_all'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/49 [00:56<13:32, 17.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping data for URL: https://www.amazon.com/Philips-Norelco-OneBlade-QP6531-70/product-reviews/B0BTZTBGS7/ref=cm_cr_arp_d_paging_btm_next_3?ie=UTF8&reviewerType=all_reviews&pageNumber=3\n",
            "Error: 'NoneType' object has no attribute 'find_all'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/49 [01:11<12:30, 16.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping data for URL: https://www.amazon.com/Philips-Norelco-OneBlade-QP6531-70/product-reviews/B0BTZTBGS7/ref=cm_cr_arp_d_paging_btm_next_4?ie=UTF8&reviewerType=all_reviews&pageNumber=4\n",
            "Error: 'NoneType' object has no attribute 'find_all'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 5/49 [01:26<11:50, 16.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping data for URL: https://www.amazon.com/Philips-Norelco-OneBlade-QP6531-70/product-reviews/B0BTZTBGS7/ref=cm_cr_arp_d_paging_btm_next_5?ie=UTF8&reviewerType=all_reviews&pageNumber=5\n",
            "Error: 'NoneType' object has no attribute 'find_all'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/49 [01:42<11:20, 15.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping data for URL: https://www.amazon.com/Philips-Norelco-OneBlade-QP6531-70/product-reviews/B0BTZTBGS7/ref=cm_cr_arp_d_paging_btm_next_6?ie=UTF8&reviewerType=all_reviews&pageNumber=6\n",
            "Error: 'NoneType' object has no attribute 'find_all'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/49 [01:57<10:56, 15.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error scraping data for URL: https://www.amazon.com/Philips-Norelco-OneBlade-QP6531-70/product-reviews/B0BTZTBGS7/ref=cm_cr_arp_d_paging_btm_next_7?ie=UTF8&reviewerType=all_reviews&pageNumber=7\n",
            "Error: 'NoneType' object has no attribute 'find_all'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/49 [01:58<11:48, 16.87s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-efb15b5559f1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get scraped data for each URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscraped_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-b1d56d68df05>\u001b[0m in \u001b[0;36mx4\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# Get the page source after waiting for dynamic content to load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}